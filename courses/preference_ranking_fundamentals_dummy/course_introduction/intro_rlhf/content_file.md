# <span style="color:#364BC9">Introduction to RLHF</span>

<video src="${PRIVATE_PREFERENCE_RANKING_VIDEO_2}" frameborder="0" allowfullscreen style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border: none; object-fit: cover;" controls="" controlslist="nodownload nofullscreen" style="width: 100%" />

## <span style="color:#364BC9">What is RLHF?</span>

<div style="border: 4px solid #d0f3f7; border-radius: 10px; padding: 20px; background-color: #d0f3f7;">Reinforcement Learning with Human Feedback (RLHF) is a method used in training Large Language Models. RLHF incorporates direct human feedback into an LLM’s learning process to guide the model’s behaviour, ensuring alignment with human preferences and ethical standards</div>

## <span style="color:#364BC9">How Does RLHF Help LLMs? </span>

<div style="border: 4px solid #d0f3f7; border-radius: 10px; padding: 20px; background-color: #d0f3f7;">RLHF improves LLMs by aligning their outputs with human values, expectations and real world applicability. It reduces harmful, biased, and irrelevant responses while enhancing usefulness and readability making LLMs more productive and trustworthy.</div>

<img height="400" width="1300" src="${PRIVATE_PREFERENCE_RANKING_IMAGE_2}" />