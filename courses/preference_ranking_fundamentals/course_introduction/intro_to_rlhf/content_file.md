# <span style="color:#364BC9">Introduction to RLHF</span>

<video src="${PRIVATE_PREFERENCE_RANKING_VIDEO_2}" frameborder="0" allowfullscreen style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border: none; object-fit: cover;" controls="" controlslist="nodownload nofullscreen" style="width: 100%" />

## What is RLHF?

Reinforcement Learning with Human Feedback (RLHF) is a method used in training Large Language Models. RLHF incorporates direct human feedback into an LLM’s learning process to guide the model’s behaviour, ensuring alignment with human preferences and ethical standards.

### How Does RLHF Help LLMs?

RLHF improves LLMs by aligning their outputs with human values, expectations and real world applicability. It reduces harmful, biased, and irrelevant responses while enhancing usefulness and readability making LLMs more productive and trustworthy.

### The Foundational RLHF Process

<img height="600" width="600" src="${PRIVATE_PREFERENCE_RANKING_IMAGE_2}" />