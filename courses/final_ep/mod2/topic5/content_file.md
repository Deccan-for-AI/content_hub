# <span style="color:#364BC9">Handling Edge Cases - What If the Model Doesnâ€™t Know?</span>

<video src="${PRIVATE_PROMPTING_VIDEO_7}" frameborder="0" allowfullscreen style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border: none; object-fit: cover;" controls="" controlslist="nodownload nofullscreen" style="width: 100%" />

### <span style="color:#364BC9">Why It Matters:</span>

In sensitive domains such as healthcare, finance, or model evaluation workflows (e.g., RLHF), inaccurate outputs can be misleading or harmful. AI models often generate guesses unless instructed otherwise.

**To improve output reliability, include a fallback clause:**

:::tip
Adding a fallback phrase improves response accuracy, reduces hallucination, and enhances safety, especially when prompting is used in high-stakes or auditable environments.
:::

### <span style="color:#364BC9">Example:</span>

* **Basic** -  *â€œSummarise expert views on AI in national security.â€*
* **Refined** - *â€œSummarise expert views based on published sources. If no consensus exists, say: â€˜Conflicting opinions found.â€™â€*

### <span style="color:#364BC9">Edge-Case Instructions:</span>

* â€œIf unsure, say: â€˜Not enough information available.â€™â€
* â€œIf the answer is uncertain, respond: â€˜Insufficient data.â€™â€
* â€œIf examples donâ€™t apply, return: â€˜No relevant match found.â€™â€
* â€œAvoid speculation. Add a disclaimer if information may be biased.â€

#### ğŸ’¡ Edge-case handling helps test a modelâ€™s ability to:

* Admit uncertainty
* Follow safety-conscious instructions
* Distinguish between knowns and unknowns