## What is Preference Ranking?

<video src="${PRIVATE_PREFERENCE_RANKING_VIDEO}" frameborder="0" allowfullscreen style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border: none; object-fit: cover;" controls="" controlslist="nodownload nofullscreen" style="width: 100%" />

With the growing use of Artificial Intelligence, Large Language Models have become increasingly important in powering chatbots, virtual assistants and other intelligent applications. While these models give impressive responses, it is crucial to consistently fine tune them through training and evaluation to maintain the quality, factuality, accuracy and relevance of outputs. This is where **Preference Ranking** becomes crucial.

Preference Ranking is a widely used method  to evaluate and compare multiple responses generated by LLMs. Responses are evaluated against a rubric and then ranked on a Likert Scale. It serves as a reward system where the model learns that responses ranked higher are good responses and it tunes itself to generate highly ranked responses in future interactions. The ranking is also followed by a justification which is backed by evidence borrowing from all the individual rubrics the response is evaluated against. This justification serves as ordered feedback that trains the model to make tailored adjustments. The model learns from detailed explanations and makes targeted changes performing better in complex output patterns.
